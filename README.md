# 동꾸기 공지 크롤러

## 프로젝트 선정 이유

2016년 겨울에 동국대학교 취업 센터에서 진행하는 iOS수업에서 팀프로젝트로 '공지사항 알림 서비스'를 구현하였습니다. 3명으로 구성된 팀에서 제가 맡은 부분은 크롤링(Web Scraping) 입니다. 당시에 Ruby 를 공부하고 있어서 루비를 이용하여 구현하였습니다. Nokogiri gem 을 이용하여 파싱 작업을 하고, Whenever gem(Worker Thread) 을 이용하여 20분마다 자동 크롤링 작업을 수행합니다. 동국대학교 내 17개의 공지사항을 페이지를 분석하여 구조를 파악했고, 오차와 중복없이 정확한 결과로 데이터를 수집하였습니다.

## 프로젝트 요약

1. 관리자 페이지 구현을 감안하여 테이블은 CRUD 가 가능하도록 생성합니다.
2. 수집할 대상이 되는 17개의 페이지의 HTML 파일을 분석하여 구현 규칙을 찾아야 합니다.
3. 제목/작성자/작성일/링크/조회수 를 수집하고 테이블에는 중복 데이터가 없어야 합니다.
4. 크롤링 작업은 30분마다 주기적으로 자동으로 수행되어야 합니다.
5. 데이터는 카테고리별로 분류되어 저장되고 조인 및 실렉트 작업을 위하여 외래키 설정이 되어 있어야 합니다.

## 프로젝트 풀이

- Nokogiri Parsing 작업

전체 페이지에 대한 정보를 각각 for 문으로 돌면서, 미리 분석해 놓은 형태대로 데이터를 파싱하고 각각의 모델(DB)에 저장합니다. uniqueness 로 제목과 링크로 중복검사를 수행하여 중복된 데이터가 없도록 합니다. Ruby on Rails에서 많은 양의 데이터를 빠른 시간 내에 저장할 경우 MVC 패턴을 이용하여 구조적으로 안정적으로 구현하는 것 보다 가장 row level 의 명령어로 직접 저장하는 것이 빠릅니다. 그렇기 때문에 ActiveRecord 에서 SQL 명령어로 직접 작업하였습니다.

- Crontab으로 자동화 구현

/app/worker/connectRequest.rb 에 수행할 작업에 대한 자세한 내용을 설정합니다. Schedule.rb 에 주기를 설정해줍니다. Whenever gem 을 이용하면, schedule.rb 가 자동으로 생성됩니다. worker 에는 rake, command 등등 여러 형태의 명령어를 입력할 수 있습니다. "$ nohup rails s &" 명령어로 서버가 켜져있는 동안 30분 단위로 요청을 반복하여 크롤링을 수행합니다.

## 프로젝트 소감

이 프로젝트는 제가 처음으로 경험한 iOS 프로젝트입니다. 프로젝트의 기획과 아이디어는 제가 제공했지만, 전체를 시간내에 구현할 자신이 없어서 팀을 구성하게 되었습니다. iOS App 부분은 다른 팀원이 맡고, 예전부터 해보고 싶던 크롤링 작업을 제가 맡게 되었습니다. 이전에 Python 으로 급하게 작업한 기억이 있는데, 처음으로 완성도 높은 크롤러를 만들었습니다.

함께 서버를 공유하고 작업하는 도중에 실수로 데이터를 통째로 날려버리는 실수를 한 적도 있는데, 백업과 복구에 대한 내용도 배울 수 있었습니다. 데이터 베이스를 다루는 방법부터 HTML 구조를 분석하여 파싱하는 방법까지 많은 공부가 되었습니다. 이 프로젝트를 경험하고 어떤 웹 페이지도 분석하여 파싱할 자신이 생겼습니다. 또한 Nokogiri parser 를 이용하여 크롤링을 하는 것에 대한 정보가 많지 않아서 공부를 하는데에 시간이 좀 걸렸습니다. 그래서 저 처럼 고생하지 않도록 예시와 함께 블로그에 튜토리얼을 만들어 포스팅 하였습니다.
